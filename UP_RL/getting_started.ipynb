{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xvu5Qvb3sYH"
      },
      "source": [
        "# **Puddle World**\n",
        "\n",
        "In this guide, we provide a step-by-step walkthrough on preparing the Puddle World environment for the Competition.\n",
        "\n",
        "Firstly, we cover the installation of the environment and related dependencies, both for local setup and utilization within a Colab notebook.\n",
        "\n",
        "Next, we detail the process of initializing the environment with desired configurations and demonstrate how to visualize its characteristics. The goal of the competition is to design an agent that does well in the five provided configurations. Therefore, we show you how to load all the various versions of the environment and inspect their characteristics.\n",
        "\n",
        "Given the objective of designing a competent agent, we present suggestions for deploying random, human, or untuned DQN agents within the environment. Observing all these different baseline behaviors serves as a valuable starting point for your work.\n",
        "\n",
        "Finally, it's time to design and train a single agent capable of superior performance across the five configurations of the environment. Testing the agent on these configurations, with the first being the default environment as described in the paper, is crucial. We provide instructions on saving results in a CSV file for submission, marking the beginning of your journey.\n",
        "\n",
        "Whether you're new to the field and eager to explore or an experienced practitioner aiming to refine your skills, this competition provides a platform to utilize your knowledge and creativity!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIhADOEiMMNG"
      },
      "source": [
        "# 1. Installing the Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfH70vPMXZDI"
      },
      "source": [
        "In this section, you can find the necessary information for installing the environment and relavant libraries for your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GMv0w4DMQIt"
      },
      "source": [
        "## 1.1. For Google Colab\n",
        "\n",
        "You can access the gym-puddle library for colab by installing the repository by the following command.\n",
        "You can also include other necessary installations of your own here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ1-mRKkhiYK",
        "outputId": "a92718cb-9244-47eb-cc71-61d16138d15a"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/Amii-Open-Source/gym-puddle.git\n",
        "#!pip install git+https://github.com/DLR-RM/stable-baselines3@f375cc393938a6b4e4dc0fb1de82b4afca37c1bd\n",
        "#!apt-get install -y xvfb x11-utils\n",
        "#!pip install pyvirtualdisplay\n",
        "#!pip install --upgrade ipykernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiDfhcj-MVoo"
      },
      "source": [
        "## 1.2. Local\n",
        "\n",
        "You can install the library locally by making a virtual environment, and installing the library by pip. The following commands show the details\n",
        "You can also find the exact versions of other libraries in the `setup.py` file in the repository.\n",
        "\n",
        "\n",
        "```\n",
        "python -m venv myenv\n",
        "source myenv/bin/activate\n",
        "pip install -e .\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIJjN7wM36mV"
      },
      "source": [
        "# 2. Introduction to the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVQx5wsZ6No"
      },
      "source": [
        "The \"Puddle World\" environment provides an interface for reinforcement learning experiments. It features puddles and challenges agents to navigate to specified goal positions while avoiding the puddles with larger negative rewards.\n",
        "You can access key details like starting position, goal location, and action noise levels. By printing these attributes, you can gain insights into the environment's layout and develop strategies for training reinforcement learning algorithms effectively within the Puddle World domain.\n",
        "Here is the default Puddle World configuration inspired by the [original paper](/http://incompleteideas.net/papers/sutton-96.pdf):\n",
        "\n",
        "**Actions**\n",
        "\n",
        "There are four actions: up, down, right, and left.\n",
        "Each action moves approximately 0.05 in these directions. For the case where an action would take the agent out of the screen limits, the action does not move the agent instead.\n",
        "\n",
        "A random gaussian noise with standard deviation 0.01 is also added to the motion along both dimensions.\n",
        "\n",
        "**Reward**\n",
        "\n",
        "The reward is -1 for each time step, plus some additional penalty if the agent gets into the puddle.\n",
        "The penalty is -400 times the distance into the puddle (distance to the nearest edge).\n",
        "\n",
        "\n",
        "**Puddle Positions**\n",
        "\n",
        "The puddles's top-left position is [0.  , 0.85] and [0.35, 0.9] respectively, and the width and height for them is [0.55, 0.2 ] and [0.2, 0.6].\n",
        "\n",
        "**Start Position**\n",
        "\n",
        "The agent starts each episode at [0.2, 0.4].\n",
        "\n",
        "**Goal Position**\n",
        "\n",
        "The episode ends succesfully if the agent reaches [1.0, 1.0] which is the goal position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-l8sBVaJgHZM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import gym_puddle\n",
        "\n",
        "from stable_baselines3 import DQN, A2C,TD3,PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines3.td3 import MlpPolicy as TD3Policy\n",
        "from stable_baselines3.a2c import MlpPolicy as A2CPolicy\n",
        "from stable_baselines3.dqn import MlpPolicy as DQNPolicy\n",
        "from stable_baselines3.ppo import MlpPolicy as PPOPolicy\n",
        "from stable_baselines3.ppo import CnnPolicy as PPOCnnPolicy\n",
        "from stable_baselines3.dqn import CnnPolicy as DQNCnnPolicy\n",
        "from stable_baselines3.a2c import CnnPolicy as A2CCnnPolicy\n",
        "\n",
        "import time\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from IPython import display\n",
        "import pyvirtualdisplay\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCZgqX8EvWLo"
      },
      "source": [
        "Here are the printed details about the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nx5pViOgfWX",
        "outputId": "0b887fb4-0cf0-4e69-a11e-58fe5c3e6b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start position: [0.2 0.4]\n",
            "goal position: [1. 1.]\n",
            "goal threshold: 0.1\n",
            "action noise: 0.01\n",
            "agent's thrust: 0.05\n",
            "puddle top left positions: [array([0.  , 0.85]), array([0.35, 0.9 ])]\n",
            "puddle widths and heights: [array([0.55, 0.2 ]), array([0.2, 0.6])]\n",
            "action space: [array([-0.05,  0.  ]), array([0.05, 0.  ]), array([ 0.  , -0.05]), array([0.  , 0.05])]\n",
            "observation space: Box(0, 255, (3, 156, 156), uint8)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"PuddleWorld-v0\")\n",
        "\n",
        "print(\"start position:\", env.get_wrapper_attr(\"start\"))\n",
        "print(\"goal position:\", env.get_wrapper_attr(\"goal\"))\n",
        "print(\"goal threshold:\", env.get_wrapper_attr(\"goal_threshold\"))\n",
        "print(\"action noise:\", env.get_wrapper_attr(\"noise\"))\n",
        "print(\"agent's thrust:\", env.get_wrapper_attr(\"thrust\"))\n",
        "print(\"puddle top left positions:\", env.get_wrapper_attr(\"puddle_top_left\"))\n",
        "print(\"puddle widths and heights:\", env.get_wrapper_attr(\"puddle_width\"))\n",
        "print(\"action space:\", env.get_wrapper_attr(\"actions\"))\n",
        "print(\"observation space:\", env.get_wrapper_attr(\"observation_space\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3zuQ4I4vdfm"
      },
      "source": [
        "Here is the visualization of the environment for the default configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_O2Y4pxziS-g"
      },
      "outputs": [],
      "source": [
        "#some functions to help the visualization and interaction wit the environment\n",
        "\n",
        "def visualize(frames, video_name = \"video.mp4\"):\n",
        "    # Saves the frames as an mp4 video using cv2\n",
        "    video_path = video_name\n",
        "    height, width, _ = frames[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    video_writer = cv2.VideoWriter(video_path, fourcc, 30, (width, height))\n",
        "    for frame in frames:\n",
        "        video_writer.write(frame)\n",
        "    video_writer.release()\n",
        "\n",
        "def online_rendering(image):\n",
        "    #Visualize one frame of the image in a display\n",
        "    ax.axis('off')\n",
        "    img_with_frame = np.zeros((image.shape[0]+2, image.shape[1]+2, 3), dtype=np.uint8)\n",
        "    img_with_frame[1:-1, 1:-1, :] = image\n",
        "    ax.imshow(img_with_frame)\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "\n",
        "def prepare_display():\n",
        "    #Prepares display for onine rendering of the frames in the game\n",
        "    _display = pyvirtualdisplay.Display(visible=False,size=(1400, 900))\n",
        "    _ = _display.start()\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "def get_action():\n",
        "    action = None\n",
        "    while action not in [\"w\", \"a\", \"s\", \"d\", \"W\", \"A\", \"S\", \"D\"]:\n",
        "        action = input(\"Enter action (w/a/s/d): \")\n",
        "    if action == \"w\":\n",
        "        return 3\n",
        "    elif action == \"a\":\n",
        "        return 0\n",
        "    elif action == \"s\":\n",
        "        return 2\n",
        "    elif action == \"d\":\n",
        "        return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "csQiaih2pfgr",
        "outputId": "1a3712db-f032-4467-fb6e-98c6eec5e982"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "obs, info = env.reset()\n",
        "image = env.render()\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "online_rendering(cv2.resize(image, (400,400)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: In the subsequent sections of this document, we have used the `prepare_display()` function, which starts a virtual display to show the online rendering of the frames of the game. This function is needed if you want to run the code on Colab, since Colab does not natively support virtual displays.\n",
        "\n",
        "However, if you are running your code on Windows, you don’t need this function. Instead, you can just rely on the `visualize()` function, which takes the list of frames as input and produces a video of the agent's performance in the game. Another way to visualize on Windows or Mac (or any system supporting virtual display windows) is to set the rendering mode to “human” at the beginning so that a window opens as the game starts and shows the online rendering.\n",
        "\n",
        "Here is an snippet of how you can do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#env = gym.make(\"PuddleWorld-v0\", render_mode=\"human\") # you should set the render_mode to \"human\" to visualize the environment locally. If you are running this code snippet on colab, these lines won't work since colab doesn't support virtual display screens\n",
        "#env.reset() #reset the environment to start a new episode\n",
        "#env.render() #this will open a window to visualize the environment\n",
        "#time.sleep(5) #wait for 5 seconds so that you can see the window\n",
        "#env.close() #you should close the environment to close the virtual window at the end of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpR--ha9O_mq"
      },
      "source": [
        "## 2.1 Accessing Different Environment Configurations\n",
        "\n",
        "Your task is to train an agent that can generalize well across different provided configurations of the environment. Each of these configurations feature different positions for puddles, which makes it challenging for the agent to find the most rewarding path to the goal.\n",
        "\n",
        "\n",
        "You can find these configurations in the `env_configs` folder of the repository. In order to access each version of the environment, you can provide the `.json` file indicating the environment details, and intitialize the puddle world as follows:\n",
        "(Note that if you are using colab, you should upload the configs to the files section of the colab)\n",
        "The puddle positions are different in these configurations, but other aspects of the environment remain the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "ZYDRdKW9O_QT",
        "outputId": "13785d2c-4f8e-4b48-c3c0-f622bca62dea"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "json_file = f\"gym_puddle/env_configs/pw1.json\"\n",
        "\n",
        "with open(json_file) as f:\n",
        "  env_setup = json.load(f)\n",
        "\n",
        "env = gym.make(\n",
        "  \"PuddleWorld-v0\",\n",
        "  start=env_setup[\"start\"],\n",
        "  goal=env_setup[\"goal\"],\n",
        "  goal_threshold=env_setup[\"goal_threshold\"],\n",
        "  noise=env_setup[\"noise\"],\n",
        "  thrust=env_setup[\"thrust\"],\n",
        "  puddle_top_left=env_setup[\"puddle_top_left\"],\n",
        "  puddle_width=env_setup[\"puddle_width\"],\n",
        ")\n",
        "\n",
        "obs, info = env.reset()\n",
        "image = env.render()\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "online_rendering(np.transpose(image, (0, 1, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2BaWj4cMaLW"
      },
      "source": [
        "# 3. Developing the Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtTuBkjbQ7MS"
      },
      "source": [
        "In preparation for the competition, we dive into the exploration of the RL framework through the introduction of three distinct agent types: random, human, and Deep Q-Network (DQN) agents.\n",
        "\n",
        "Each agent represents varying levels of sophistication and learning abilities within the Puddle World environment. This comprehensive investigation allows us to gain valuable insights into key aspects of the environment as well as exploring RL-related methods such as the utilization of deep reinforcement learning strategies, and related libraries.\n",
        "\n",
        "Through this exploration, you can familiarize yourself with the environment, grasp the details of the training loop, and become proficient in utilizing essential libraries such as Stable Baselines. This could be a starting point for building your own agent, either using existing libraries, and modifying them to your desires, or implementing new ideas from cutting-edge RL research!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rew9gS45hi6"
      },
      "source": [
        "## 3.3. DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAcvJ3mSlMT2"
      },
      "source": [
        "In this section, a  [ Deep Q-Network (DQN) agent](https://arxiv.org/abs/1312.5602) is utilized to interact with the Puddle World environment. The code demonstrates how to set up training with [Stable Baselines](https://stable-baselines3.readthedocs.io/en/master/), a popular library for reinforcement learning. The DQN model is initialized and trained using default hyperparameters, but you can explore and adjust these hyperparameters for optimal performance. After training, the trained model is loaded for evaluation. The environment is initialized, and the DQN agent's interaction with the environment is visualized. The agent selects actions based on the learned policy, and the environment responds accordingly. This process continues until the episode terminates. At the end of the episode, the total reward earned by the agent is displayed, and the sequence of frames captured during the interaction is saved as a video for further analysis. While the provided code serves as a starting point for training a DQN agent with stable baselines if you wish to use this library, you are encouraged to experiment with different hyperparameters to improve the agent's performance in navigating the complex dynamics of the Puddle World environment. We encourage you to explore more algorithms in the same library or build your agents from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['pw1.json', 'pw2.json', 'pw3.json', 'pw4.json', 'pw5.json', 'pw6.json'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Define the path to the JSON file\n",
        "file_path = 'gym_puddle/env_configs'\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "all_files = os.listdir(file_path)\n",
        "\n",
        "# Filter the list to include only JSON files\n",
        "json_files = [file for file in all_files if file.endswith('.json')]\n",
        "\n",
        "all_data = {}\n",
        "\n",
        "# Loop through the JSON files, open each one, and load its content\n",
        "for json_file in json_files:\n",
        "    file_path_json = os.path.join(file_path, json_file)\n",
        "    with open(file_path_json, 'r') as file:\n",
        "        # Load the JSON content\n",
        "        data = json.load(file)\n",
        "        # Store the data using the filename as the key\n",
        "        all_data[json_file] = data\n",
        "all_data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_all = []\n",
        "import random\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "def make_env(env_id, config):\n",
        "    def _init():\n",
        "        env = Monitor(gym.make(env_id, **config))\n",
        "        env.reset(seed = 0)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "for key, value in all_data.items():\n",
        "        if key == \"pw6.jspn\":\n",
        "             continue\n",
        "        else:\n",
        "            env = {\"env_id\" : \"PuddleWorld-v0\", \n",
        "                \"config\" : {\"start\":value[\"start\"],\n",
        "                \"goal\":value[\"goal\"],\n",
        "                \"goal_threshold\":value[\"goal_threshold\"],\n",
        "                \"noise\":value[\"noise\"],\n",
        "                \"thrust\":value[\"thrust\"],\n",
        "                \"puddle_top_left\":value[\"puddle_top_left\"],\n",
        "                \"puddle_width\":value[\"puddle_width\"]\n",
        "                }}\n",
        "            env_all.append(env)\n",
        "env_all_vec = DummyVecEnv([make_env(env[\"env_id\"],env[\"config\"]) for env in env_all])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
        "    \"\"\"\n",
        "    Linear learning rate schedule.\n",
        "\n",
        "    :param initial_value: Initial learning rate.\n",
        "    :return: schedule that computes\n",
        "      current learning rate depending on remaining progress\n",
        "    \"\"\"\n",
        "    def func(progress_remaining: float) -> float:\n",
        "        \"\"\"\n",
        "        Progress will decrease from 1 (beginning) to 0.\n",
        "\n",
        "        :param progress_remaining:\n",
        "        :return: current learning rate\n",
        "        \"\"\"\n",
        "        if progress_remaining <=0.5 and progress_remaining > 0.25:\n",
        "            return initial_value/10\n",
        "        elif progress_remaining <=0.25:\n",
        "            return initial_value/100\n",
        "        elif progress_remaining <= 0.75 and progress_remaining> 0.5:\n",
        "            return initial_value/2\n",
        "        else:\n",
        "            return initial_value\n",
        "        \n",
        "    return func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define the exponential learning rate schedule\n",
        "def exp_schedule(initial_lr, decay_rate, decay_steps):\n",
        "    \"\"\"\n",
        "    Exponential decay schedule function.\n",
        "\n",
        "    :param initial_lr: (float) The initial learning rate\n",
        "    :param decay_rate: (float) The decay rate\n",
        "    :param decay_steps: (int) Number of steps for each decay\n",
        "    :return: (function)\n",
        "    \"\"\"\n",
        "    def schedule(progress):\n",
        "        current_step = (1 - progress) * decay_steps\n",
        "        return initial_lr * np.exp(-decay_rate * current_step)\n",
        "\n",
        "    return schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from gymnasium import spaces\n",
        "\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    :param observation_space: (gym.Space)\n",
        "    :param features_dim: (int) Number of features extracted.\n",
        "        This corresponds to the number of unit for the last layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: spaces.Box, features_dim: int = 256):\n",
        "        super().__init__(observation_space, features_dim)\n",
        "        # We assume CxHxW images (channels first)\n",
        "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()\n",
        "            ).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "\n",
        "        return self.linear(self.cnn(observations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.sb2_compat.rmsprop_tf_like import RMSpropTFLike\n",
        "\n",
        "env_eval = Monitor(gym.make(env_all[1][\"env_id\"],**env_all[1][\"config\"]))\n",
        "env_eval.reset(seed = 0)\n",
        "#env = make_vec_env(env_all[2][\"env_id\"], env_kwargs=env_all[2][\"config\"],n_envs=4)\n",
        "\n",
        "\n",
        "eval_callback = EvalCallback(env_eval, best_model_save_path=\"./logs/\",\n",
        "                             log_path=\"./logs/\", eval_freq=10000,n_eval_episodes =2,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CustomCNN,\n",
        "    features_extractor_kwargs=dict(features_dim=512),\n",
        "    net_arch = [256, 256]\n",
        ")\n",
        "\n",
        "\n",
        "DQN_model = DQN(DQNCnnPolicy, env_all_vec, verbose=1, seed = 0, gamma = 0.9, optimize_memory_usage=True, \n",
        "                replay_buffer_kwargs={'handle_timeout_termination': False},\n",
        "                policy_kwargs=policy_kwargs,batch_size=1024, train_freq=10, \n",
        "                gradient_steps=1,max_grad_norm=1.0, exploration_fraction=0.2, exploration_initial_eps=1.0, exploration_final_eps=0.005,\n",
        "                learning_rate=0.00009, buffer_size=50000)\n",
        "\n",
        "DQN_model = DQN_model.load('best_model_3.zip', env=env_all_vec,)\n",
        "\n",
        "#PPO_model.learn(total_timesteps=2000000,callback=eval_callback)\n",
        "#DQN_model.learn(total_timesteps=10000,callback=eval_callback)\n",
        "#DQN_model.save(\"NEXT_BEST_2.zip\")\n",
        "# for j in range(len(env_all_vec.envs)):\n",
        "#         env = make_vec_env(env_all[j][\"env_id\"], env_kwargs=env_all[j][\"config\"],n_envs=4)\n",
        "#         env.reset()\n",
        "#         PPO_model.set_env(env)\n",
        "#         PPO_model.learn(total_timesteps=1000000)\n",
        "#         env.close()\n",
        "\n",
        "#DQN_model.save('new_best_model_final.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUI_JsgOtmYM",
        "outputId": "077b4a90-aaaa-4add-a2fe-734959d54a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here\n",
            "total reward in this episode: -26\n",
            "episode 0: reward: -26\n"
          ]
        }
      ],
      "source": [
        "dqn_model = DQN_model\n",
        "env_num = 2\n",
        "env = gym.make(env_all[env_num][\"env_id\"],**env_all[env_num][\"config\"])\n",
        "\n",
        "obs, info = env.reset(seed=1)\n",
        "\n",
        "# Create an empty list to store the frames\n",
        "frames = []\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(1):\n",
        "  total_reward = 0\n",
        "  done = False\n",
        "  num_steps = 0\n",
        "\n",
        "  while not done and num_steps <=1000: # to avoid infinite loops for the untuned DQN we set a truncation limit, but you should make your agent sophisticated enough to avoid infinite-step episodes\n",
        "      num_steps +=1\n",
        "      action, _states = dqn_model.predict(obs)\n",
        "      obs, reward, done, trunc, info = env.step(action)\n",
        "      total_reward += reward\n",
        "      if done == True:\n",
        "        print(\"here\")\n",
        "\n",
        "      image = env.render()\n",
        "      frames.append(image)\n",
        "\n",
        "      if done:\n",
        "        print(f\"total reward in this episode: {total_reward}\")\n",
        "        episode_rewards.append(total_reward)\n",
        "        total_reward = 0\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "\n",
        "if episode_rewards == []:\n",
        "  print(\"no episode finished in this run.\")\n",
        "else:\n",
        "  for i, reward in enumerate(episode_rewards):\n",
        "    print(f\"episode {i}: reward: {reward}\")\n",
        "\n",
        "visualize(frames, \"DQN_\" + str(env_num)+\".mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -37\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -39\n",
            "here\n",
            "total reward in this episode: -38\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -37\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -274.27701082945424\n",
            "episode 0: reward: -33\n",
            "episode 1: reward: -31\n",
            "episode 2: reward: -35\n",
            "episode 3: reward: -33\n",
            "episode 4: reward: -35\n",
            "episode 5: reward: -36\n",
            "episode 6: reward: -36\n",
            "episode 7: reward: -36\n",
            "episode 8: reward: -37\n",
            "episode 9: reward: -36\n",
            "episode 10: reward: -39\n",
            "episode 11: reward: -38\n",
            "episode 12: reward: -34\n",
            "episode 13: reward: -35\n",
            "episode 14: reward: -30\n",
            "episode 15: reward: -29\n",
            "episode 16: reward: -35\n",
            "episode 17: reward: -32\n",
            "episode 18: reward: -36\n",
            "episode 19: reward: -35\n",
            "episode 20: reward: -34\n",
            "episode 21: reward: -36\n",
            "episode 22: reward: -35\n",
            "episode 23: reward: -30\n",
            "episode 24: reward: -32\n",
            "episode 25: reward: -36\n",
            "episode 26: reward: -30\n",
            "episode 27: reward: -32\n",
            "episode 28: reward: -32\n",
            "episode 29: reward: -34\n",
            "episode 30: reward: -31\n",
            "episode 31: reward: -35\n",
            "episode 32: reward: -35\n",
            "episode 33: reward: -33\n",
            "episode 34: reward: -35\n",
            "episode 35: reward: -34\n",
            "episode 36: reward: -34\n",
            "episode 37: reward: -34\n",
            "episode 38: reward: -34\n",
            "episode 39: reward: -34\n",
            "episode 40: reward: -36\n",
            "episode 41: reward: -36\n",
            "episode 42: reward: -33\n",
            "episode 43: reward: -33\n",
            "episode 44: reward: -37\n",
            "episode 45: reward: -32\n",
            "episode 46: reward: -36\n",
            "episode 47: reward: -34\n",
            "episode 48: reward: -32\n",
            "episode 49: reward: -31\n",
            "episode 50: reward: -28\n",
            "episode 51: reward: -35\n",
            "episode 52: reward: -34\n",
            "episode 53: reward: -30\n",
            "episode 54: reward: -33\n",
            "episode 55: reward: -32\n",
            "episode 56: reward: -32\n",
            "episode 57: reward: -34\n",
            "episode 58: reward: -36\n",
            "episode 59: reward: -33\n",
            "episode 60: reward: -33\n",
            "episode 61: reward: -36\n",
            "episode 62: reward: -31\n",
            "episode 63: reward: -32\n",
            "episode 64: reward: -32\n",
            "episode 65: reward: -31\n",
            "episode 66: reward: -30\n",
            "episode 67: reward: -31\n",
            "episode 68: reward: -36\n",
            "episode 69: reward: -36\n",
            "episode 70: reward: -33\n",
            "episode 71: reward: -30\n",
            "episode 72: reward: -35\n",
            "episode 73: reward: -34\n",
            "episode 74: reward: -34\n",
            "episode 75: reward: -32\n",
            "episode 76: reward: -34\n",
            "episode 77: reward: -31\n",
            "episode 78: reward: -34\n",
            "episode 79: reward: -32\n",
            "episode 80: reward: -32\n",
            "episode 81: reward: -34\n",
            "episode 82: reward: -35\n",
            "episode 83: reward: -32\n",
            "episode 84: reward: -33\n",
            "episode 85: reward: -36\n",
            "episode 86: reward: -34\n",
            "episode 87: reward: -33\n",
            "episode 88: reward: -34\n",
            "episode 89: reward: -33\n",
            "episode 90: reward: -36\n",
            "episode 91: reward: -36\n",
            "episode 92: reward: -34\n",
            "episode 93: reward: -34\n",
            "episode 94: reward: -32\n",
            "episode 95: reward: -32\n",
            "episode 96: reward: -33\n",
            "episode 97: reward: -35\n",
            "episode 98: reward: -33\n",
            "episode 99: reward: -274.27701082945424\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -37\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -37\n",
            "here\n",
            "total reward in this episode: -38\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -270.2770108294542\n",
            "episode 0: reward: -30\n",
            "episode 1: reward: -31\n",
            "episode 2: reward: -35\n",
            "episode 3: reward: -30\n",
            "episode 4: reward: -35\n",
            "episode 5: reward: -36\n",
            "episode 6: reward: -36\n",
            "episode 7: reward: -34\n",
            "episode 8: reward: -37\n",
            "episode 9: reward: -36\n",
            "episode 10: reward: -37\n",
            "episode 11: reward: -38\n",
            "episode 12: reward: -34\n",
            "episode 13: reward: -35\n",
            "episode 14: reward: -30\n",
            "episode 15: reward: -31\n",
            "episode 16: reward: -35\n",
            "episode 17: reward: -30\n",
            "episode 18: reward: -36\n",
            "episode 19: reward: -35\n",
            "episode 20: reward: -34\n",
            "episode 21: reward: -35\n",
            "episode 22: reward: -35\n",
            "episode 23: reward: -30\n",
            "episode 24: reward: -34\n",
            "episode 25: reward: -36\n",
            "episode 26: reward: -30\n",
            "episode 27: reward: -32\n",
            "episode 28: reward: -32\n",
            "episode 29: reward: -32\n",
            "episode 30: reward: -31\n",
            "episode 31: reward: -35\n",
            "episode 32: reward: -35\n",
            "episode 33: reward: -33\n",
            "episode 34: reward: -31\n",
            "episode 35: reward: -34\n",
            "episode 36: reward: -34\n",
            "episode 37: reward: -34\n",
            "episode 38: reward: -36\n",
            "episode 39: reward: -34\n",
            "episode 40: reward: -34\n",
            "episode 41: reward: -36\n",
            "episode 42: reward: -33\n",
            "episode 43: reward: -33\n",
            "episode 44: reward: -35\n",
            "episode 45: reward: -30\n",
            "episode 46: reward: -36\n",
            "episode 47: reward: -34\n",
            "episode 48: reward: -34\n",
            "episode 49: reward: -33\n",
            "episode 50: reward: -30\n",
            "episode 51: reward: -35\n",
            "episode 52: reward: -36\n",
            "episode 53: reward: -32\n",
            "episode 54: reward: -33\n",
            "episode 55: reward: -32\n",
            "episode 56: reward: -32\n",
            "episode 57: reward: -34\n",
            "episode 58: reward: -34\n",
            "episode 59: reward: -33\n",
            "episode 60: reward: -33\n",
            "episode 61: reward: -30\n",
            "episode 62: reward: -31\n",
            "episode 63: reward: -32\n",
            "episode 64: reward: -33\n",
            "episode 65: reward: -31\n",
            "episode 66: reward: -30\n",
            "episode 67: reward: -34\n",
            "episode 68: reward: -36\n",
            "episode 69: reward: -34\n",
            "episode 70: reward: -33\n",
            "episode 71: reward: -30\n",
            "episode 72: reward: -35\n",
            "episode 73: reward: -32\n",
            "episode 74: reward: -32\n",
            "episode 75: reward: -34\n",
            "episode 76: reward: -34\n",
            "episode 77: reward: -32\n",
            "episode 78: reward: -34\n",
            "episode 79: reward: -32\n",
            "episode 80: reward: -32\n",
            "episode 81: reward: -30\n",
            "episode 82: reward: -33\n",
            "episode 83: reward: -32\n",
            "episode 84: reward: -31\n",
            "episode 85: reward: -36\n",
            "episode 86: reward: -34\n",
            "episode 87: reward: -33\n",
            "episode 88: reward: -32\n",
            "episode 89: reward: -33\n",
            "episode 90: reward: -36\n",
            "episode 91: reward: -34\n",
            "episode 92: reward: -32\n",
            "episode 93: reward: -34\n",
            "episode 94: reward: -32\n",
            "episode 95: reward: -34\n",
            "episode 96: reward: -33\n",
            "episode 97: reward: -33\n",
            "episode 98: reward: -34\n",
            "episode 99: reward: -270.2770108294542\n",
            "here\n",
            "total reward in this episode: -23\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -23\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -23\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -23\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -23\n",
            "here\n",
            "total reward in this episode: -23\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -23\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -23\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -23\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -262.2276427138087\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -24\n",
            "episode 0: reward: -23\n",
            "episode 1: reward: -25\n",
            "episode 2: reward: -26\n",
            "episode 3: reward: -25\n",
            "episode 4: reward: -25\n",
            "episode 5: reward: -27\n",
            "episode 6: reward: -29\n",
            "episode 7: reward: -23\n",
            "episode 8: reward: -28\n",
            "episode 9: reward: -27\n",
            "episode 10: reward: -26\n",
            "episode 11: reward: -28\n",
            "episode 12: reward: -26\n",
            "episode 13: reward: -26\n",
            "episode 14: reward: -24\n",
            "episode 15: reward: -25\n",
            "episode 16: reward: -25\n",
            "episode 17: reward: -24\n",
            "episode 18: reward: -28\n",
            "episode 19: reward: -24\n",
            "episode 20: reward: -27\n",
            "episode 21: reward: -28\n",
            "episode 22: reward: -25\n",
            "episode 23: reward: -24\n",
            "episode 24: reward: -26\n",
            "episode 25: reward: -26\n",
            "episode 26: reward: -24\n",
            "episode 27: reward: -25\n",
            "episode 28: reward: -26\n",
            "episode 29: reward: -26\n",
            "episode 30: reward: -26\n",
            "episode 31: reward: -24\n",
            "episode 32: reward: -27\n",
            "episode 33: reward: -25\n",
            "episode 34: reward: -25\n",
            "episode 35: reward: -25\n",
            "episode 36: reward: -26\n",
            "episode 37: reward: -26\n",
            "episode 38: reward: -26\n",
            "episode 39: reward: -24\n",
            "episode 40: reward: -25\n",
            "episode 41: reward: -26\n",
            "episode 42: reward: -25\n",
            "episode 43: reward: -25\n",
            "episode 44: reward: -27\n",
            "episode 45: reward: -23\n",
            "episode 46: reward: -27\n",
            "episode 47: reward: -25\n",
            "episode 48: reward: -26\n",
            "episode 49: reward: -24\n",
            "episode 50: reward: -23\n",
            "episode 51: reward: -27\n",
            "episode 52: reward: -26\n",
            "episode 53: reward: -25\n",
            "episode 54: reward: -25\n",
            "episode 55: reward: -25\n",
            "episode 56: reward: -23\n",
            "episode 57: reward: -23\n",
            "episode 58: reward: -25\n",
            "episode 59: reward: -27\n",
            "episode 60: reward: -26\n",
            "episode 61: reward: -24\n",
            "episode 62: reward: -26\n",
            "episode 63: reward: -26\n",
            "episode 64: reward: -23\n",
            "episode 65: reward: -24\n",
            "episode 66: reward: -24\n",
            "episode 67: reward: -26\n",
            "episode 68: reward: -28\n",
            "episode 69: reward: -23\n",
            "episode 70: reward: -24\n",
            "episode 71: reward: -24\n",
            "episode 72: reward: -25\n",
            "episode 73: reward: -26\n",
            "episode 74: reward: -26\n",
            "episode 75: reward: -25\n",
            "episode 76: reward: -26\n",
            "episode 77: reward: -24\n",
            "episode 78: reward: -27\n",
            "episode 79: reward: -23\n",
            "episode 80: reward: -26\n",
            "episode 81: reward: -25\n",
            "episode 82: reward: -24\n",
            "episode 83: reward: -26\n",
            "episode 84: reward: -25\n",
            "episode 85: reward: -27\n",
            "episode 86: reward: -27\n",
            "episode 87: reward: -24\n",
            "episode 88: reward: -26\n",
            "episode 89: reward: -25\n",
            "episode 90: reward: -27\n",
            "episode 91: reward: -27\n",
            "episode 92: reward: -24\n",
            "episode 93: reward: -26\n",
            "episode 94: reward: -262.2276427138087\n",
            "episode 95: reward: -26\n",
            "episode 96: reward: -26\n",
            "episode 97: reward: -24\n",
            "episode 98: reward: -29\n",
            "episode 99: reward: -24\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -221.18473843915402\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -213.41814623081373\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -226.03450530054613\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -29\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -28\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -25\n",
            "here\n",
            "total reward in this episode: -26\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -27\n",
            "here\n",
            "total reward in this episode: -24\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -28\n",
            "episode 0: reward: -30\n",
            "episode 1: reward: -28\n",
            "episode 2: reward: -30\n",
            "episode 3: reward: -27\n",
            "episode 4: reward: -30\n",
            "episode 5: reward: -28\n",
            "episode 6: reward: -32\n",
            "episode 7: reward: -25\n",
            "episode 8: reward: -30\n",
            "episode 9: reward: -27\n",
            "episode 10: reward: -28\n",
            "episode 11: reward: -33\n",
            "episode 12: reward: -221.18473843915402\n",
            "episode 13: reward: -27\n",
            "episode 14: reward: -27\n",
            "episode 15: reward: -26\n",
            "episode 16: reward: -31\n",
            "episode 17: reward: -26\n",
            "episode 18: reward: -31\n",
            "episode 19: reward: -27\n",
            "episode 20: reward: -30\n",
            "episode 21: reward: -30\n",
            "episode 22: reward: -25\n",
            "episode 23: reward: -28\n",
            "episode 24: reward: -28\n",
            "episode 25: reward: -34\n",
            "episode 26: reward: -27\n",
            "episode 27: reward: -28\n",
            "episode 28: reward: -28\n",
            "episode 29: reward: -28\n",
            "episode 30: reward: -27\n",
            "episode 31: reward: -28\n",
            "episode 32: reward: -31\n",
            "episode 33: reward: -29\n",
            "episode 34: reward: -27\n",
            "episode 35: reward: -29\n",
            "episode 36: reward: -27\n",
            "episode 37: reward: -29\n",
            "episode 38: reward: -29\n",
            "episode 39: reward: -26\n",
            "episode 40: reward: -27\n",
            "episode 41: reward: -26\n",
            "episode 42: reward: -25\n",
            "episode 43: reward: -27\n",
            "episode 44: reward: -213.41814623081373\n",
            "episode 45: reward: -26\n",
            "episode 46: reward: -29\n",
            "episode 47: reward: -27\n",
            "episode 48: reward: -30\n",
            "episode 49: reward: -26\n",
            "episode 50: reward: -28\n",
            "episode 51: reward: -31\n",
            "episode 52: reward: -27\n",
            "episode 53: reward: -30\n",
            "episode 54: reward: -28\n",
            "episode 55: reward: -226.03450530054613\n",
            "episode 56: reward: -25\n",
            "episode 57: reward: -24\n",
            "episode 58: reward: -27\n",
            "episode 59: reward: -29\n",
            "episode 60: reward: -31\n",
            "episode 61: reward: -28\n",
            "episode 62: reward: -28\n",
            "episode 63: reward: -26\n",
            "episode 64: reward: -27\n",
            "episode 65: reward: -26\n",
            "episode 66: reward: -28\n",
            "episode 67: reward: -28\n",
            "episode 68: reward: -30\n",
            "episode 69: reward: -28\n",
            "episode 70: reward: -29\n",
            "episode 71: reward: -29\n",
            "episode 72: reward: -28\n",
            "episode 73: reward: -30\n",
            "episode 74: reward: -27\n",
            "episode 75: reward: -28\n",
            "episode 76: reward: -28\n",
            "episode 77: reward: -28\n",
            "episode 78: reward: -29\n",
            "episode 79: reward: -26\n",
            "episode 80: reward: -27\n",
            "episode 81: reward: -27\n",
            "episode 82: reward: -27\n",
            "episode 83: reward: -34\n",
            "episode 84: reward: -25\n",
            "episode 85: reward: -31\n",
            "episode 86: reward: -30\n",
            "episode 87: reward: -31\n",
            "episode 88: reward: -27\n",
            "episode 89: reward: -28\n",
            "episode 90: reward: -31\n",
            "episode 91: reward: -32\n",
            "episode 92: reward: -25\n",
            "episode 93: reward: -26\n",
            "episode 94: reward: -30\n",
            "episode 95: reward: -31\n",
            "episode 96: reward: -27\n",
            "episode 97: reward: -24\n",
            "episode 98: reward: -30\n",
            "episode 99: reward: -28\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -37\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -38\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -37\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -37\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -37\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -31\n",
            "here\n",
            "total reward in this episode: -36\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -30\n",
            "here\n",
            "total reward in this episode: -35\n",
            "here\n",
            "total reward in this episode: -32\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -33\n",
            "here\n",
            "total reward in this episode: -34\n",
            "here\n",
            "total reward in this episode: -32\n",
            "episode 0: reward: -31\n",
            "episode 1: reward: -31\n",
            "episode 2: reward: -32\n",
            "episode 3: reward: -33\n",
            "episode 4: reward: -37\n",
            "episode 5: reward: -32\n",
            "episode 6: reward: -34\n",
            "episode 7: reward: -30\n",
            "episode 8: reward: -34\n",
            "episode 9: reward: -31\n",
            "episode 10: reward: -32\n",
            "episode 11: reward: -38\n",
            "episode 12: reward: -34\n",
            "episode 13: reward: -32\n",
            "episode 14: reward: -35\n",
            "episode 15: reward: -35\n",
            "episode 16: reward: -31\n",
            "episode 17: reward: -30\n",
            "episode 18: reward: -34\n",
            "episode 19: reward: -35\n",
            "episode 20: reward: -34\n",
            "episode 21: reward: -36\n",
            "episode 22: reward: -33\n",
            "episode 23: reward: -32\n",
            "episode 24: reward: -37\n",
            "episode 25: reward: -36\n",
            "episode 26: reward: -30\n",
            "episode 27: reward: -34\n",
            "episode 28: reward: -34\n",
            "episode 29: reward: -34\n",
            "episode 30: reward: -31\n",
            "episode 31: reward: -31\n",
            "episode 32: reward: -37\n",
            "episode 33: reward: -30\n",
            "episode 34: reward: -33\n",
            "episode 35: reward: -34\n",
            "episode 36: reward: -32\n",
            "episode 37: reward: -34\n",
            "episode 38: reward: -32\n",
            "episode 39: reward: -31\n",
            "episode 40: reward: -34\n",
            "episode 41: reward: -31\n",
            "episode 42: reward: -30\n",
            "episode 43: reward: -33\n",
            "episode 44: reward: -33\n",
            "episode 45: reward: -34\n",
            "episode 46: reward: -33\n",
            "episode 47: reward: -34\n",
            "episode 48: reward: -32\n",
            "episode 49: reward: -31\n",
            "episode 50: reward: -30\n",
            "episode 51: reward: -37\n",
            "episode 52: reward: -33\n",
            "episode 53: reward: -30\n",
            "episode 54: reward: -33\n",
            "episode 55: reward: -33\n",
            "episode 56: reward: -34\n",
            "episode 57: reward: -30\n",
            "episode 58: reward: -34\n",
            "episode 59: reward: -33\n",
            "episode 60: reward: -33\n",
            "episode 61: reward: -34\n",
            "episode 62: reward: -33\n",
            "episode 63: reward: -32\n",
            "episode 64: reward: -30\n",
            "episode 65: reward: -31\n",
            "episode 66: reward: -32\n",
            "episode 67: reward: -31\n",
            "episode 68: reward: -36\n",
            "episode 69: reward: -32\n",
            "episode 70: reward: -33\n",
            "episode 71: reward: -35\n",
            "episode 72: reward: -33\n",
            "episode 73: reward: -34\n",
            "episode 74: reward: -34\n",
            "episode 75: reward: -30\n",
            "episode 76: reward: -36\n",
            "episode 77: reward: -34\n",
            "episode 78: reward: -33\n",
            "episode 79: reward: -30\n",
            "episode 80: reward: -30\n",
            "episode 81: reward: -32\n",
            "episode 82: reward: -33\n",
            "episode 83: reward: -34\n",
            "episode 84: reward: -31\n",
            "episode 85: reward: -33\n",
            "episode 86: reward: -34\n",
            "episode 87: reward: -31\n",
            "episode 88: reward: -32\n",
            "episode 89: reward: -31\n",
            "episode 90: reward: -36\n",
            "episode 91: reward: -34\n",
            "episode 92: reward: -32\n",
            "episode 93: reward: -30\n",
            "episode 94: reward: -35\n",
            "episode 95: reward: -32\n",
            "episode 96: reward: -34\n",
            "episode 97: reward: -33\n",
            "episode 98: reward: -34\n",
            "episode 99: reward: -32\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dqn_model = DQN_model\n",
        "test_results = {'env_1': [], 'env_2': [], 'env_3': [], 'env_4': [], 'env_5': []}\n",
        "for env_num in range(5):\n",
        "  env = gym.make(env_all[env_num][\"env_id\"],**env_all[env_num][\"config\"])\n",
        "\n",
        "  obs, info = env.reset()\n",
        "\n",
        "  # Create an empty list to store the frames\n",
        "  frames = []\n",
        "  episode_rewards = []\n",
        "\n",
        "  for episode in range(100):\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    num_steps = 0\n",
        "\n",
        "    while not done and num_steps <=1000: # to avoid infinite loops for the untuned DQN we set a truncation limit, but you should make your agent sophisticated enough to avoid infinite-step episodes\n",
        "        num_steps +=1\n",
        "        action, _states = dqn_model.predict(obs)\n",
        "        obs, reward, done, trunc, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done == True:\n",
        "          print(\"here\")\n",
        "\n",
        "        image = env.render()\n",
        "        frames.append(image)\n",
        "\n",
        "        if done:\n",
        "          print(f\"total reward in this episode: {total_reward}\")\n",
        "          episode_rewards.append(total_reward)\n",
        "          total_reward = 0\n",
        "          break\n",
        "    env.reset(seed = episode)\n",
        "  env.close()\n",
        "\n",
        "  if episode_rewards == []:\n",
        "    print(\"no episode finished in this run.\")\n",
        "  else:\n",
        "    for i, reward in enumerate(episode_rewards):\n",
        "      print(f\"episode {i}: reward: {reward}\")\n",
        "\n",
        "  visualize(frames, \"DQN_\" + str(env_num)+\".mp4\")\n",
        "\n",
        "  name_of_env = \"env_\" + str(env_num+1)\n",
        "  test_results[name_of_env] = episode_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_results['env_5'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvpUay3y5kYW"
      },
      "source": [
        "#4. Now It's Your Turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoUfMFHCl70O"
      },
      "source": [
        "Now, you're invited to dive into the world of reinforcement learning and take control of the Puddle World environment. We encourage you to become an active participant by modifying the provided code, tweaking parameters, and experimenting with different strategies. You can come up with your own agent ideas, or use pre-exisiting libraries, and adapt them to the problem.\n",
        "\n",
        "\n",
        "The goal is to build **a single agent that can generalize well across all the environment configurations.**\n",
        "Once you have built your agent, test your trained agent with 100 different seeds, each seed for one episode, in the five provided configurations, save the total reward in each episode for each configuration, and submit the results as a `.csv` file as indicated in the Kaggle platform. You can set different seeds for the environment with `env.reset(seed = n)`) with `n` being the number from 1 to 100.\n",
        "\n",
        " Whether you're a beginner eager to explore or an experienced practitioner seeking to refine your skills, this competition offers an opportunity to apply your knowledge and creativity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je02_uxV9N0c"
      },
      "source": [
        "## 4.1. Submission Format\n",
        "\n",
        "Here you can find a sample submission. Imagine that you have the episodic rewards saved in a list as shown in the below code. You can make a `submission.csv` file with the provided format as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAYlx07NY7sa",
        "outputId": "78bb77c1-65b2-48f6-872e-874ceecd99c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved successfully to submission.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a list of 100 episodic rewards for each configuration\n",
        "# Example data structure: [(config1_rewards), (config2_rewards), ...]\n",
        "\n",
        "# Replace this example data with your actual rewards data\n",
        "episode_rewards_per_configuration = [\n",
        "     test_results['env_1'],  # Configuration 1 rewards\n",
        "    test_results['env_2'],   # Configuration 2 rewards\n",
        "    test_results['env_3'],  # Configuration 3 rewards\n",
        "    test_results['env_4'],  # Configuration 4 rewards\n",
        "   test_results['env_5'],   # Configuration 5 rewards\n",
        "]\n",
        "\n",
        "# Define the column names\n",
        "columns = ['seed_ID', 'ep_reward_pw1', 'ep_reward_pw2', 'ep_reward_pw3', 'ep_reward_pw4', 'ep_reward_pw5']\n",
        "\n",
        "# Create a list of dictionaries to store data\n",
        "data = []\n",
        "\n",
        "# Populate the list with episode IDs and rewards\n",
        "for episode_id in range(1, 101):  # Assuming 100 episodes\n",
        "    row_data = {'seed_ID': episode_id}\n",
        "    for i, rewards in enumerate(episode_rewards_per_configuration):\n",
        "        row_data[columns[i + 1]] = rewards[episode_id - 1]\n",
        "    data.append(row_data)\n",
        "\n",
        "# Create DataFrame from the list of dictionaries\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the file name for saving the results\n",
        "csv_file_name = \"submission.csv\"\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(csv_file_name, index=False)\n",
        "\n",
        "print(\"Results saved successfully to\", csv_file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1mwZltVat-9",
        "outputId": "fc069386-ed4e-4daf-ab23-c458b0e93efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    seed_ID  ep_reward_pw1  ep_reward_pw2  ep_reward_pw3  ep_reward_pw4  \\\n",
            "0         1     -33.000000     -30.000000          -23.0          -30.0   \n",
            "1         2     -31.000000     -31.000000          -25.0          -28.0   \n",
            "2         3     -35.000000     -35.000000          -26.0          -30.0   \n",
            "3         4     -33.000000     -30.000000          -25.0          -27.0   \n",
            "4         5     -35.000000     -35.000000          -25.0          -30.0   \n",
            "..      ...            ...            ...            ...            ...   \n",
            "95       96     -32.000000     -34.000000          -26.0          -31.0   \n",
            "96       97     -33.000000     -33.000000          -26.0          -27.0   \n",
            "97       98     -35.000000     -33.000000          -24.0          -24.0   \n",
            "98       99     -33.000000     -34.000000          -29.0          -30.0   \n",
            "99      100    -274.277011    -270.277011          -24.0          -28.0   \n",
            "\n",
            "    ep_reward_pw5  \n",
            "0             -31  \n",
            "1             -31  \n",
            "2             -32  \n",
            "3             -33  \n",
            "4             -37  \n",
            "..            ...  \n",
            "95            -32  \n",
            "96            -34  \n",
            "97            -33  \n",
            "98            -34  \n",
            "99            -32  \n",
            "\n",
            "[100 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file name for loading the results\n",
        "csv_file_name = \"submission.csv\"\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(csv_file_name)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHLrXbuf7FH4"
      },
      "source": [
        "# 6. Fun Expedition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwxRkDNivkUi"
      },
      "source": [
        "You can also modify the properties of the environment such as the initial state, puddle locations, and more. You can play around with the environment and challenge your agent to tackle the hardest versions of this environment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "EkxQLUa_ib18",
        "outputId": "e85d34c3-d611-4e4e-e12b-1100cc9e3ae5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\n",
        "        \"PuddleWorld-v0\",\n",
        "        start=[0.5,0.5],\n",
        "        goal=[1.,1.],\n",
        "        goal_threshold=0.1,\n",
        "        noise=0.01,\n",
        "        thrust=0.05,\n",
        "        puddle_top_left=[[0.1  , 0.8],[0.5, 0.1 ]],\n",
        "        puddle_width=[[0.1, 0.1 ],[0.3, 0.1]]\n",
        "    )\n",
        "\n",
        "obs, info = env.reset()\n",
        "image = env.render()\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "online_rendering(image)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
